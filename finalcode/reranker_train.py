"""
HotpotQA Cross-Encoder Reranker Training Script
Binary Cross Entropy Loss ë²„ì „ (Hard Negatives Mining ì œì™¸)
- ë‹¨ìˆœ BCE Loss ì‚¬ìš©
- ìµœì í™”: 24ì‹œê°„ ë‚´ í•™ìŠµ ì™„ë£Œ
- FP32 ì „ìš© (Titan XP ìµœì í™”)
"""

import pickle
import logging
import math
from pathlib import Path
from typing import List, Tuple
from datasets import Dataset
import torch

from sentence_transformers import InputExample, CrossEncoder
from sentence_transformers.cross_encoder import (
    CrossEncoderTrainer,
    CrossEncoderTrainingArguments
)
from sentence_transformers.cross_encoder.evaluation import (
    CEBinaryClassificationEvaluator
)
from sentence_transformers.cross_encoder.losses.BinaryCrossEntropyLoss import BinaryCrossEntropyLoss

# ============================================================================
# ì„¤ì • ë° ë¡œê¹…
# ============================================================================

logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# ê²½ë¡œ ì„¤ì •
TRAIN_DATA_PATH = Path("train_samples_with_title.pkl")
OUTPUT_DIR = Path("hotpotqa_reranker_title_full")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
MODEL_NAME = "cross-encoder/ms-marco-MiniLM-L12-v2"
BATCH_SIZE = 8
GRADIENT_ACCUMULATION = 2
NUM_EPOCHS = 4
LEARNING_RATE = 2e-5
WARMUP_RATIO = 0.1
VAL_SPLIT_RATIO = 0.1
MAX_LENGTH = 512

# ë°ì´í„° ìƒ˜í”Œë§ ì„¤ì •
USE_DATA_SAMPLING = False
SAMPLING_RATIO = 0.5

# Mixed Precision (FP32 ê³ ì •)
USE_FP16 = False
USE_BF16 = False

# ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
CHECKPOINT_SAVE_STEPS = 6000
CHECKPOINT_SAVE_LIMIT = 2


# ============================================================================
# ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
# ============================================================================

def load_training_data(pickle_path: Path) -> List[InputExample]:
    """Pickle íŒŒì¼ì—ì„œ í•™ìŠµ ë°ì´í„° ë¡œë“œ"""
    if not pickle_path.exists():
        raise FileNotFoundError(f"í•™ìŠµ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pickle_path}")
    
    logger.info(f"ğŸ“‚ í•™ìŠµ ë°ì´í„° ë¡œë”©: {pickle_path}")
    
    with open(pickle_path, "rb") as f:
        train_samples = pickle.load(f)
    
    logger.info(f"âœ“ ì´ {len(train_samples):,}ê°œì˜ ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ")
    
    return train_samples


def sample_balanced_data(
    samples: List[InputExample],
    sampling_ratio: float = 0.5,
    seed: int = 42
) -> List[InputExample]:
    """Positive/Negative ë¹„ìœ¨ ìœ ì§€í•˜ë©´ì„œ ìƒ˜í”Œë§"""
    import random
    random.seed(seed)
    
    positive_samples = [s for s in samples if s.label == 1.0]
    negative_samples = [s for s in samples if s.label == 0.0]
    
    pos_sample_size = int(len(positive_samples) * sampling_ratio)
    neg_sample_size = int(len(negative_samples) * sampling_ratio)
    
    sampled_positive = random.sample(positive_samples, pos_sample_size)
    sampled_negative = random.sample(negative_samples, neg_sample_size)
    
    sampled_data = sampled_positive + sampled_negative
    random.shuffle(sampled_data)
    
    logger.info("âœ‚ï¸  ë°ì´í„° ìƒ˜í”Œë§ ì™„ë£Œ:")
    logger.info(f"   - ì›ë³¸: {len(samples):,}")
    logger.info(f"   - ìƒ˜í”Œë§ í›„: {len(sampled_data):,} ({sampling_ratio:.0%})")
    logger.info(f"   - Positive: {len(sampled_positive):,}")
    logger.info(f"   - Negative: {len(sampled_negative):,}")
    
    return sampled_data


def analyze_data_distribution(samples: List[InputExample]) -> dict:
    """ë°ì´í„° ë¶„í¬ ë¶„ì„"""
    positive_count = sum(1 for s in samples if s.label == 1.0)
    negative_count = len(samples) - positive_count
    
    stats = {
        "total": len(samples),
        "positive": positive_count,
        "negative": negative_count,
        "pos_ratio": positive_count / len(samples) if samples else 0
    }
    
    logger.info("ğŸ“Š ë°ì´í„° ë¶„í¬:")
    logger.info(f"   - ì´ ìƒ˜í”Œ: {stats['total']:,}")
    logger.info(f"   - Positive: {stats['positive']:,}")
    logger.info(f"   - Negative: {stats['negative']:,}")
    logger.info(f"   - Positive ë¹„ìœ¨: {stats['pos_ratio']:.1%}")
    
    return stats


def create_train_val_split(
    samples: List[InputExample],
    val_ratio: float = 0.05,
    seed: int = 42
) -> Tuple[List[InputExample], List[InputExample]]:
    """í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• """
    import random
    random.seed(seed)
    
    positive_samples = [s for s in samples if s.label == 1.0]
    negative_samples = [s for s in samples if s.label == 0.0]
    
    random.shuffle(positive_samples)
    random.shuffle(negative_samples)
    
    val_pos_size = int(len(positive_samples) * val_ratio)
    val_neg_size = int(len(negative_samples) * val_ratio)
    
    val_samples = positive_samples[:val_pos_size] + negative_samples[:val_neg_size]
    train_samples = positive_samples[val_pos_size:] + negative_samples[val_neg_size:]
    
    random.shuffle(train_samples)
    random.shuffle(val_samples)
    
    logger.info("ğŸ”€ Train/Val ë¶„í•  ì™„ë£Œ:")
    logger.info(f"   - í•™ìŠµ ë°ì´í„°: {len(train_samples):,}ê°œ")
    logger.info(f"   - ê²€ì¦ ë°ì´í„°: {len(val_samples):,}ê°œ")
    
    return train_samples, val_samples


# ============================================================================
# ëª¨ë¸ ë° í•™ìŠµ ì„¤ì •
# ============================================================================

def check_gpu_info():
    """GPU ì •ë³´ í™•ì¸"""
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        
        logger.info("ğŸ–¥ï¸  GPU ì •ë³´:")
        logger.info(f"   - ì¥ì¹˜: {device_name}")
        logger.info(f"   - ì´ ë©”ëª¨ë¦¬: {total_memory:.1f} GB")
        logger.info(f"   - CUDA ë²„ì „: {torch.version.cuda}")
        logger.info(f"   - Mixed Precision: FP32")
    else:
        logger.warning("âš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


def create_training_arguments(output_dir: Path) -> CrossEncoderTrainingArguments:
    """í•™ìŠµ ì„¤ì • ìƒì„±"""
    
    logger.info("ğŸ¯ í•™ìŠµ ì„¤ì •:")
    logger.info(f"   - Loss: Binary Cross Entropy")
    logger.info(f"   - Precision: FP32")
    logger.info(f"   - Batch Size: {BATCH_SIZE}")
    logger.info(f"   - Gradient Accumulation: {GRADIENT_ACCUMULATION}")
    logger.info(f"   - Effective Batch: {BATCH_SIZE * GRADIENT_ACCUMULATION}")
    logger.info(f"   - Epochs: {NUM_EPOCHS}")
    logger.info(f"   - Data Sampling: {'Yes' if USE_DATA_SAMPLING else 'No'} ({SAMPLING_RATIO:.0%})")
    
    args = CrossEncoderTrainingArguments(
        output_dir=str(output_dir),
        
        # ===== Mixed Precision =====
        fp16=False,
        bf16=False,
        
        # ===== ë°°ì¹˜ ì„¤ì • =====
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION,
        per_device_eval_batch_size=BATCH_SIZE * 2,
        
        # ===== í•™ìŠµ íŒŒë¼ë¯¸í„° =====
        num_train_epochs=NUM_EPOCHS,
        learning_rate=LEARNING_RATE,
        warmup_ratio=WARMUP_RATIO,
        weight_decay=0.01,
        max_grad_norm=1.0,
        
        # ===== í‰ê°€ ë° ì €ì¥ =====
        eval_strategy="steps",
        eval_steps=CHECKPOINT_SAVE_STEPS,
        save_strategy="steps",
        save_steps=CHECKPOINT_SAVE_STEPS,
        save_total_limit=CHECKPOINT_SAVE_LIMIT,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        
        # ===== ë¡œê¹… =====
        logging_dir=str(output_dir / "logs"),
        logging_steps=200,
        logging_first_step=True,
        report_to=[],
        
        # ===== ë°ì´í„° ë¡œë” =====
        dataloader_num_workers=4,
        dataloader_pin_memory=True,
        dataloader_drop_last=False,
        dataloader_prefetch_factor=2,
        
        # ===== ì¬í˜„ì„± =====
        seed=42,
        data_seed=42,
    )
    
    return args


# ============================================================================
# í•™ìŠµ ì‹¤í–‰
# ============================================================================

def train_cross_encoder(
    train_samples: List[InputExample],
    val_samples: List[InputExample]
) -> CrossEncoder:
    """Cross-Encoder í•™ìŠµ (BCE Loss, Hard Negatives ì œì™¸)"""
    
    logger.info("="*80)
    logger.info("ğŸš€ Cross-Encoder í•™ìŠµ ì‹œì‘ (BCE Loss)")
    logger.info("="*80)
    
    # 1. ëª¨ë¸ ë¡œë“œ
    logger.info(f"ğŸ“¦ ëª¨ë¸ ë¡œë“œ: {MODEL_NAME}")
    model = CrossEncoder(
        MODEL_NAME,
        num_labels=1,  # BCEëŠ” ë‹¨ì¼ ì¶œë ¥
        max_length=MAX_LENGTH,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    #datasetìœ¼ë¡œ ë³€í™˜
    logger.info("ğŸ”„ ë°ì´í„° í˜•ì‹ ë³€í™˜ ì¤‘...")

    train_data = {
        "sentence1": [sample.texts[0] for sample in train_samples],  # query
        "sentence2": [sample.texts[1] for sample in train_samples],  # document
        "label": [float(sample.label) for sample in train_samples]   # 1.0 or 0.0
    }
    train_dataset = Dataset.from_dict(train_data)
    
    val_data = {
        "sentence1": [sample.texts[0] for sample in val_samples],
        "sentence2": [sample.texts[1] for sample in val_samples],
        "label": [float(sample.label) for sample in val_samples]
    }
    val_dataset = Dataset.from_dict(val_data)
    
    logger.info(f"âœ“ Train dataset: {len(train_dataset):,}ê°œ")
    logger.info(f"âœ“ Val dataset: {len(val_dataset):,}ê°œ")

    # 2. BCE Loss ì •ì˜
    # pos_weightëŠ” ë°ì´í„°ì…‹ì˜ positive:negative ë¹„ìœ¨ë¡œ ì„¤ì •
    positive_count = sum(1 for s in train_samples if s.label == 1.0)
    negative_count = len(train_samples) - positive_count
    pos_weight_value = negative_count / positive_count if positive_count > 0 else 1.0
    
    loss = BinaryCrossEntropyLoss(
        model=model,
        pos_weight=torch.tensor(pos_weight_value)
    )
    logger.info(f"ğŸ“ BCE Loss ì„¤ì •")
    logger.info(f"   - Positive: {positive_count:,}")
    logger.info(f"   - Negative: {negative_count:,}")
    logger.info(f"   - pos_weight: {pos_weight_value:.2f}")
    
    # 3. Evaluator ìƒì„±
    logger.info("ğŸ“ Evaluator ì„¤ì •")
    evaluator = CEBinaryClassificationEvaluator.from_input_examples(
        val_samples,
        name='HotpotQA-val',
        write_csv=True
    )
    
    # ì´ˆê¸° ì„±ëŠ¥ í‰ê°€
    logger.info("ğŸ“Š ì´ˆê¸° ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
    try:
        initial_score = evaluator(model, output_path=str(OUTPUT_DIR / "initial_eval"))
        logger.info(f"âœ“ ì´ˆê¸° í‰ê°€ ì ìˆ˜: {initial_score:.4f}")
    except Exception as e:
        logger.warning(f"ì´ˆê¸° í‰ê°€ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ì§„í–‰): {e}")
        initial_score = None
    
    # 4. í•™ìŠµ ì„¤ì •
    args = create_training_arguments(OUTPUT_DIR)
    
    # 5. Trainer ì´ˆê¸°í™”
    logger.info("ğŸ“ Trainer ì´ˆê¸°í™”")
    trainer = CrossEncoderTrainer(
        model=model,
        args=args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        loss=loss,  # BCE Loss ì „ë‹¬
        evaluator=evaluator,
    )
    
    # 6. ë©”ëª¨ë¦¬ í™•ì¸
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        allocated = torch.cuda.memory_allocated() / 1e9
        logger.info(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {allocated:.2f} GB")
    
    # 7. í•™ìŠµ ì‹œê°„ ì˜ˆì¸¡
    total_samples = len(train_dataset)
    effective_batch = args.per_device_train_batch_size * args.gradient_accumulation_steps
    steps_per_epoch = math.ceil(total_samples / effective_batch)
    total_steps = steps_per_epoch * NUM_EPOCHS
    
    samples_per_second = 13.5
    est_time_hours = (total_samples * NUM_EPOCHS / samples_per_second) / 3600
    
    logger.info("â±ï¸  í•™ìŠµ ì˜ˆìƒ ì •ë³´:")
    logger.info(f"   - ì´ ìŠ¤í…: {total_steps:,}")
    logger.info(f"   - ì´ ìƒ˜í”Œ: {total_samples:,}")
    logger.info(f"   - ì‹¤ì§ˆ ë°°ì¹˜: {effective_batch}")
    logger.info(f"   - ì˜ˆìƒ ì‹œê°„: {est_time_hours:.1f}ì‹œê°„ ({est_time_hours/24:.1f}ì¼)")
    
    # 8. í•™ìŠµ ì‹œì‘
    logger.info("\n" + "="*80)
    logger.info("ğŸ‹ï¸  í•™ìŠµ ì‹œì‘")
    logger.info("="*80 + "\n")
    
    trainer.train()
    
    # 9. ìµœì¢… í‰ê°€
    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š ìµœì¢… ëª¨ë¸ í‰ê°€")
    logger.info("="*80)
    
    try:
        final_score = evaluator(model, output_path=str(OUTPUT_DIR / "final_eval"))
        logger.info(f"âœ“ ìµœì¢… í‰ê°€ ì ìˆ˜: {final_score:.4f}")
        
        if initial_score is not None:
            improvement = final_score - initial_score
            logger.info(f"âœ“ ì„±ëŠ¥ í–¥ìƒ: {improvement:+.4f} ({improvement/abs(initial_score)*100:+.1f}%)")
    except Exception as e:
        logger.warning(f"ìµœì¢… í‰ê°€ ì‹¤íŒ¨: {e}")
    
    # 10. ìµœì¢… ëª¨ë¸ ì €ì¥
    final_model_path = OUTPUT_DIR / "final"
    final_model_path.mkdir(exist_ok=True, parents=True)
    model.save(str(final_model_path))
    logger.info(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")
    
    return model


# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    logger.info("\n" + "="*80)
    logger.info("ğŸ HotpotQA Cross-Encoder with BCE Loss (ë‹¨ìˆœ ë²„ì „)")
    logger.info("="*80 + "\n")
    
    try:
        # 1. GPU í™•ì¸
        check_gpu_info()
        
        # 2. ë°ì´í„° ë¡œë“œ
        train_samples = load_training_data(TRAIN_DATA_PATH)
        
        # 3. ë°ì´í„° ìƒ˜í”Œë§
        if USE_DATA_SAMPLING:
            train_samples = sample_balanced_data(
                train_samples,
                sampling_ratio=SAMPLING_RATIO
            )
        
        # 4. ë°ì´í„° ë¶„ì„
        stats = analyze_data_distribution(train_samples)
        
        if stats['pos_ratio'] < 0.1 or stats['pos_ratio'] > 0.9:
            logger.warning(f"âš ï¸  ë°ì´í„° ë¶ˆê· í˜• ê°ì§€: Positive ë¹„ìœ¨ {stats['pos_ratio']:.1%}")
        
        # 5. Train/Val ë¶„í• 
        train_data, val_data = create_train_val_split(
            train_samples,
            val_ratio=VAL_SPLIT_RATIO
        )
        
        # 6. í•™ìŠµ ì‹¤í–‰
        model = train_cross_encoder(
            train_samples=train_data,
            val_samples=val_data
        )
        
        # 7. ì™„ë£Œ ë©”ì‹œì§€
        logger.info("\n" + "="*80)
        logger.info("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
        logger.info("="*80)
        logger.info(f"ğŸ“ ëª¨ë¸: {OUTPUT_DIR / 'final'}")
        logger.info(f"ğŸ“Š TensorBoard: {OUTPUT_DIR / 'logs'}")
        logger.info("\nğŸ’¡ ì‚¬ìš© ë°©ë²•:")
        logger.info("  from sentence_transformers import CrossEncoder")
        logger.info(f"  model = CrossEncoder('{OUTPUT_DIR / 'final'}')")
        logger.info("  scores = model.predict([['query', 'document']])")
        
        logger.info("\nğŸ“‹ ì„¤ì • ìš”ì•½:")
        logger.info(f"  - Loss Function: Binary Cross Entropy")
        logger.info(f"  - Hard Negatives: No (ì œì™¸)")
        logger.info(f"  - Epochs: {NUM_EPOCHS}")
        logger.info(f"  - Data Sampling: {SAMPLING_RATIO:.0%}")
        logger.info(f"  - ì˜ˆìƒ ì‹œê°„: 15-18ì‹œê°„ (Titan XP ê¸°ì¤€)")
        
    except FileNotFoundError as e:
        logger.error(f"âŒ íŒŒì¼ ì—†ìŒ: {e}")
        logger.info("\nğŸ’¡ í•´ê²° ë°©ë²•:")
        logger.info("  1. ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”")
        logger.info("  2. train_samples.pkl íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”")
        
    except Exception as e:
        logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(traceback.format_exc())


if __name__ == "__main__":
    main()
