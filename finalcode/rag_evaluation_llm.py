import os
import re
import torch
from pathlib import Path
import json
from transformers import AutoModelForCausalLM, AutoTokenizer
import numpy as np  # [ì¶”ê°€] ëˆ„ë½ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬
from tqdm import tqdm # [ì¶”ê°€] ëˆ„ë½ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬



def load_llama_judge(model_name: str = 'meta-llama/Llama-3.2-3B-Instruct'):
    """
    ë¡œì»¬/ì„œë²„ í™˜ê²½ì—ì„œ Llama Judge ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    print(f'ğŸ¤– Loading {model_name}...')

    # [ìˆ˜ì •] float32 -> bfloat16 (ë©”ëª¨ë¦¬ ì ˆì•½ ë° ì†ë„ í–¥ìƒ)
    # GPUê°€ Ampere ì•„í‚¤í…ì²˜(RTX 30xx, A100 ë“±) ì´ìƒì´ë©´ bfloat16, ê·¸ ì™¸ì—” float16 ê¶Œì¥
    dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=dtype,
        device_map='auto' if torch.cuda.is_available() else None,
    )
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
    )
    
    # [ì¶”ê°€] Pad Token ì„¤ì • (ê²½ê³  ë°©ì§€)
    if tokenizer.pad_token is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id

    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    print(f'âœ“ Model loaded (dtype: {dtype})')
    return model, tokenizer


def llm_judge_accuracy_llama(query: str, generated_answer: str, ground_truth: str, model, tokenizer, max_new_tokens: int = 256):
    """
    Llamaë¥¼ ì‚¬ìš©í•œ ë‹µë³€ ì •í™•ë„ í‰ê°€ (0-10ì )
    """

    prompt = f"""You are an expert evaluator for question-answering systems.

Rate the accuracy of the predicted answer compared to the ground truth answer on a scale of 0-10.

Scoring guidelines:
- 10: Perfect match or semantically identical
- 8-9: Correct with minor wording differences
- 6-7: Mostly correct but missing some details
- 4-5: Partially correct
- 2-3: Incorrect but somewhat related
- 0-1: Completely wrong or unrelated

Question: {query}
Ground Truth: {ground_truth}
Predicted Answer: {generated_answer}

Provide your evaluation in this exact format:
Score: [number from 0-10]
Reasoning: [brief explanation in 1-2 sentences]
"""

    messages = [
        {'role': 'system', 'content': 'You are a precise and objective evaluator.'},
        {'role': 'user', 'content': prompt}
    ]

    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(input_text, return_tensors='pt').to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.1,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)

    # [ìˆ˜ì •] ì ìˆ˜ íŒŒì‹± ë¡œì§ ê°•í™” (Markdown bold ì²˜ë¦¬ ë“± ëŒ€ì‘)
    score_match = re.search(r'Score:\s*\*?(\d+(?:\.\d+)?)\*?', response, re.IGNORECASE)
    score = float(score_match.group(1)) if score_match else 0.0 # ëª» ì°¾ìœ¼ë©´ 0ì  ì²˜ë¦¬ (ì•ˆì „í•˜ê²Œ)
    score = max(0.0, min(10.0, score))

    reasoning_match = re.search(r'Reasoning:\s*(.+?)(?:\n|$)', response, re.IGNORECASE | re.DOTALL)
    reasoning = reasoning_match.group(1).strip() if reasoning_match else response.strip()

    return {'score': score, 'reasoning': reasoning, 'raw_response': response}


def evaluate_qa_accuracy_llama(json_path: str, output_path: str, model_name: str = 'meta-llama/Llama-3.2-3B-Instruct'):
    """
    Llamaë¥¼ ì‚¬ìš©í•œ QA ì •í™•ë„ í‰ê°€
    """
    # [ì¶”ê°€] ë¬¸ìì—´ ê²½ë¡œë¥¼ Path ê°ì²´ë¡œ ë³€í™˜
    json_path = Path(json_path)
    output_path = Path(output_path)

    print('='*80)
    print('ğŸ¤– LLM Judge - Llama Evaluation')
    print('='*80)
    print(f'Model: {model_name}')
    print(f'Input: {json_path}')
    print(f'Output: {output_path}')
    print('='*80)

    model, tokenizer = load_llama_judge(model_name)

    with open(json_path, 'r', encoding='utf-8') as f:
        qa_records = json.load(f)

    print(f'âœ“ Loaded {len(qa_records)} QA records')

    evaluated_results = []

    for idx, record in enumerate(tqdm(qa_records, desc='Evaluating')):
        # [ì¤‘ìš” ìˆ˜ì •] ì´ì „ ë‹¨ê³„(RAG Pipeline)ì˜ JSON Keyì™€ ì¼ì¹˜ì‹œí‚´
        # ì´ì „ ì½”ë“œ ì €ì¥ í‚¤: "question", "predicted_answer", "ground_truth"
        query = record.get('question', record.get('query', '')) 
        generated_answer = record.get('predicted_answer', record.get('generated_answer', record.get('answer', '')))
        ground_truth = record.get('ground_truth', '')

        try:
            accuracy_eval = llm_judge_accuracy_llama(query, generated_answer, ground_truth, model, tokenizer)
        except Exception as e:
            print(f'Error at index {idx}: {e}')
            accuracy_eval = {'score': 0.0, 'reasoning': f'Error: {str(e)}', 'raw_response': ''}

        evaluated_results.append({
            'id': idx,
            'question': query,
            'predicted_answer': generated_answer,
            'ground_truth': ground_truth,
            'accuracy_score': accuracy_eval['score'],
            'reasoning': accuracy_eval['reasoning']
        })

        # ì¤‘ê°„ ì €ì¥ (10ê°œë§ˆë‹¤)
        if (idx + 1) % 10 == 0:
            with open(output_path.with_suffix('.tmp.json'), 'w', encoding='utf-8') as f:
                json.dump(evaluated_results, f, ensure_ascii=False, indent=2)

    # ìµœì¢… ì €ì¥
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(evaluated_results, f, ensure_ascii=False, indent=2)

    scores = [r['accuracy_score'] for r in evaluated_results]

    print('' + '='*80)
    print('ğŸ“Š Evaluation Summary')
    print('='*80)
    if scores:
        print(f'Total samples: {len(scores)}')
        print(f'Average score: {np.mean(scores):.2f} / 10')
        print(f'Std deviation: {np.std(scores):.2f}')
        print(f'Min score: {min(scores):.2f}')
        print(f'Max score: {max(scores):.2f}')
    else:
        print("No scores to evaluate.")
    print('='*80)
    print(f'âœ… Results saved to: {output_path}')

    return evaluated_results

# %% ì‹¤í–‰ ì˜ˆì‹œ
# ê²½ë¡œê°€ ë§ëŠ”ì§€ í™•ì¸ í›„ ì‹¤í–‰í•˜ì„¸ìš”.
if __name__ == "__main__":
    # 1. Baseline í‰ê°€
    # 2. Ours í‰ê°€
    if os.path.exists('title_full_results.json'): # íŒŒì¼ëª… í™•ì¸ í•„ìš” (ì˜ˆ: rag_baseline_results.jsonê³¼ êµ¬ë¶„ì´ í•„ìš”í•˜ë‹¤ë©´)
        results_ours = evaluate_qa_accuracy_llama(
            json_path='title_full_results.json', # íŒŒì¼ëª…ì„ ë³¸ì¸ì˜ ì„¤ì •ì— ë§ê²Œ ë³€ê²½í•˜ì„¸ìš”
            output_path='evaluation_results_title_full.json',
            model_name='meta-llama/Llama-3.2-1B-Instruct'
        )
