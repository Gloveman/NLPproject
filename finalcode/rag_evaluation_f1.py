import json
import re
import string
import collections
import numpy as np
from pathlib import Path
from tqdm import tqdm

# ==========================================
# 1. í‘œì¤€ ì •ê·œí™” ë° ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜ (ë³€ë™ ì—†ìŒ)
# ==========================================

def normalize_answer(s):
    """ë‹µë³€ ì •ê·œí™”"""
    def remove_articles(text):
        return re.sub(r'\b(a|an|the)\b', ' ', text)
    def white_space_fix(text):
        return ' '.join(text.split())
    def remove_punc(text):
        exclude = set(string.punctuation)
        return ''.join(ch for ch in text if ch not in exclude)
    def lower(text):
        return text.lower()
    return white_space_fix(remove_articles(remove_punc(lower(str(s)))))

def compute_exact_match(prediction, ground_truth):
    return int(normalize_answer(prediction) == normalize_answer(ground_truth))

def compute_f1(prediction, ground_truth):
    pred_tokens = normalize_answer(prediction).split()
    truth_tokens = normalize_answer(ground_truth).split()
    
    common = collections.Counter(pred_tokens) & collections.Counter(truth_tokens)
    num_same = sum(common.values())
    
    if len(pred_tokens) == 0 or len(truth_tokens) == 0:
        return int(pred_tokens == truth_tokens)
    
    precision = 1.0 * num_same / len(pred_tokens)
    recall = 1.0 * num_same / len(truth_tokens)
    
    if precision + recall == 0:
        return 0
    
    f1 = (2 * precision * recall) / (precision + recall)
    return f1

# ==========================================
# 2. í‰ê°€ ì‹¤í–‰ í•¨ìˆ˜ (JSON ì €ì¥ ì œê±°)
# ==========================================

def evaluate_standard_metrics(json_path: str):
    """
    JSON íŒŒì¼ì„ ì½ì–´ EMê³¼ F1 Score í†µê³„ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    json_path = Path(json_path)
    
    print('\n' + '='*80)
    print(f'ğŸ“ Standard Metrics Evaluation: {json_path.name}')
    print('='*80)
    
    if not json_path.exists():
        print(f"âŒ Error: File not found - {json_path}")
        return

    # ë°ì´í„° ë¡œë“œ
    with open(json_path, 'r', encoding='utf-8') as f:
        qa_records = json.load(f)

    em_scores = []
    f1_scores = []

    # í‰ê°€ ë£¨í”„
    for record in tqdm(qa_records, desc="Calculating"):
        # Key ìœ ì—° ì²˜ë¦¬
        query = record.get('question', record.get('query', ''))
        pred = record.get('predicted_answer', record.get('generated_answer', record.get('answer', '')))
        truth = record.get('ground_truth', '')
        
        # ì ìˆ˜ ê³„ì‚°
        em = compute_exact_match(pred, truth)
        f1 = compute_f1(pred, truth)
        
        em_scores.append(em)
        f1_scores.append(f1)

    # í‰ê·  ê³„ì‚°
    avg_em = np.mean(em_scores) * 100
    avg_f1 = np.mean(f1_scores) * 100
    
    # í†µê³„ ì¶œë ¥
    print('-' * 80)
    print(f'âœ… Total Samples   : {len(qa_records)}')
    print(f'ğŸ¯ Exact Match (EM): {avg_em:.2f}%')
    print(f'âš–ï¸  F1 Score        : {avg_f1:.2f}%')
    print('='*80)
        
    return {
        "em": avg_em,
        "f1": avg_f1
    }

# ==========================================
# 3. ë©”ì¸ ì‹¤í–‰
# ==========================================

if __name__ == "__main__":
    # Baseline í‰ê°€
    if Path('rag_baseline_results.json').exists():
        evaluate_standard_metrics('rag_baseline_results.json')
        
    # Ours í‰ê°€
    if Path('title_full_results.json').exists():
        evaluate_standard_metrics('rag_ours_results.json')
